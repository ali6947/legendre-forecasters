{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Legendre Memory Unit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYahiHxsgZil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "754c8de6-83e3-4c12-993d-c3afc7ace1b7"
      },
      "source": [
        "!pip install nengolib"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nengolib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/00/62bbce813d135da3799f4afc26957d942d4ae27085fb0df1bfb57dcf692b/nengolib-0.5.2-py2.py3-none-any.whl (117kB)\n",
            "\r\u001b[K     |██▉                             | 10kB 26.7MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 20kB 32.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 30kB 20.3MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 40kB 24.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 51kB 23.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 61kB 25.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 71kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 81kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 92kB 17.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 102kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 112kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 17.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.7/dist-packages (from nengolib) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from nengolib) (1.4.1)\n",
            "Collecting nengo<3.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/ce/e314e1176bfbbe6c3b6cf4e8fa0620cafad8f8bad04203c55881e9cb2fb0/nengo-2.8.0-py2.py3-none-any.whl (375kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 28.5MB/s \n",
            "\u001b[?25hInstalling collected packages: nengo, nengolib\n",
            "Successfully installed nengo-2.8.0 nengolib-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAiRtAIA3kM4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sympy.matrices import Matrix, eye, zeros, ones, diag, GramSchmidt\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "from nengolib.signal import Identity, cont2discrete\n",
        "from nengolib.synapses import LegendreDelay\n",
        "from functools import partial\n",
        "\n",
        "def lecun_uniform(tensor):\n",
        "    fan_in = nn.init._calculate_correct_fan(tensor, 'fan_in')\n",
        "    nn.init.uniform_(tensor, -math.sqrt(3 / fan_in), math.sqrt(3 / fan_in))\n",
        "    \n",
        "class LegendreMemoryUnitCell(nn.Module):\n",
        "  def __init__(self, input_dim, units , order, theta,\n",
        "                 input_encoders_initializer=lecun_uniform,\n",
        "                 hidden_encoders_initializer=lecun_uniform,\n",
        "                 memory_encoders_initializer=partial(torch.nn.init.constant_, val=0),\n",
        "                 input_kernel_initializer=torch.nn.init.xavier_normal_,\n",
        "                 hidden_kernel_initializer=torch.nn.init.xavier_normal_,\n",
        "                #  hidden_kernel_initializer=torch.nn.init.uniform_,\n",
        "                 include_bias=False, #added by ali acc to branch\n",
        "                 memory_kernel_initializer=torch.nn.init.xavier_normal_):\n",
        "    super(LegendreMemoryUnitCell, self).__init__()\n",
        "\n",
        "    self.order = order\n",
        "    self.theta = theta\n",
        "    self.units = units\n",
        "    self.include_bias=include_bias\n",
        "\n",
        "    realizer = Identity()\n",
        "    self._realizer_result = realizer(LegendreDelay(theta=theta, order=self.order))\n",
        "\n",
        "    self._ss = cont2discrete(self._realizer_result.realization, dt=1., method='zoh')\n",
        "\n",
        "    self._A = self._ss.A - np.eye(order)\n",
        "    self._B = self._ss.B\n",
        "    self._C = self._ss.C\n",
        "\n",
        "    self.AT = nn.Parameter(torch.Tensor(self._A), requires_grad=False)\n",
        "    self.BT = nn.Parameter(torch.Tensor(self._B), requires_grad=False)\n",
        "\n",
        "    if self.include_bias:\n",
        "      self.bias = nn.Parameter(torch.Tensor(1, self.units),requires_grad=True) #added by ali for the fix given in github issue\n",
        "\n",
        "    self.encoder_input = nn.Parameter(torch.Tensor(1,input_dim), requires_grad=True)\n",
        "    self.encoder_hidden = nn.Parameter(torch.Tensor(1,self.units), requires_grad=True)\n",
        "    self.encoder_memory = nn.Parameter(torch.Tensor(1,self.order ), requires_grad=True)\n",
        "    self.kernel_input = nn.Parameter(torch.Tensor(self.units, input_dim), requires_grad=True)\n",
        "    self.kernel_hidden = nn.Parameter(torch.Tensor(self.units, self.units), requires_grad=True)\n",
        "    self.kernel_memory = nn.Parameter(torch.Tensor(self.units, self.order), requires_grad=True)\n",
        "    \n",
        "    if self.include_bias:\n",
        "      torch.nn.init.constant_(self.bias, 0)\n",
        "\n",
        "    input_encoders_initializer(self.encoder_input)\n",
        "    hidden_encoders_initializer(self.encoder_hidden)\n",
        "    memory_encoders_initializer(self.encoder_memory)\n",
        "    input_kernel_initializer(self.kernel_input)\n",
        "    hidden_kernel_initializer(self.kernel_hidden)\n",
        "    memory_kernel_initializer(self.kernel_memory)\n",
        "\n",
        "  def EulerOdeSolver(self):\n",
        "    A_hat = (self.step_delta_t/self.theta)*self.AT + torch.eye(self.order,self.d_order_ode)\n",
        "    B_hat = (self.step_delta_t/self.theta)*self.BT\n",
        "\n",
        "    return A_hat, B_hat\n",
        "\n",
        "  def forward(self, xt, states):\n",
        "    ht, mt = states\n",
        "\n",
        "    ut = F.linear(xt, self.encoder_input)+F.linear(ht, self.encoder_hidden)+  F.linear(mt, self.encoder_memory)\n",
        "\n",
        "\n",
        "    mt = mt + F.linear(mt, self.AT) + F.linear(ut, self.BT)\n",
        "\n",
        "    ht = nn.Tanh()(F.linear(xt, self.kernel_input) + F.linear(ht, self.kernel_hidden) + F.linear(mt, self.kernel_memory)+(self.bias if self.include_bias else 0))\n",
        "    \n",
        "    return ht, (ht, mt)\n",
        "\n",
        "class LegendreMemoryUnit(nn.Module):\n",
        "  def __init__(self, input_dim, units , order, theta):\n",
        "    super(LegendreMemoryUnit, self).__init__()\n",
        "\n",
        "    self.units = units\n",
        "    self.order = order\n",
        "\n",
        "    self.lmucell = LegendreMemoryUnitCell(input_dim, units , order, theta,include_bias=False)\n",
        "\n",
        "  def forward(self, xt):\n",
        "    outputs = []\n",
        "    \n",
        "    h0 = torch.zeros(xt.size(0),self.units).cuda()\n",
        "    m0 = torch.zeros(xt.size(0),self.order).cuda()\n",
        "    states = (h0,m0)\n",
        "    for i in range(xt.size(1)):\n",
        "      out, states = self.lmucell(xt[:,i,:], states)\n",
        "      outputs += [out]\n",
        "    return torch.stack(outputs).permute(1,0,2), states\n",
        "    # return torch.stack(outputs).permute(1,0,2), states\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRC62TebNYWJ"
      },
      "source": [
        "# Test LMU output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfTlZrsfbSXH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ef3b565-e1cd-4831-c6aa-bd16c9890919"
      },
      "source": [
        "x = torch.rand(64,5000,1).to(torch.device(\"cuda:0\"))\n",
        "h0 = torch.rand(64,49).to(torch.device(\"cuda:0\"))\n",
        "m0 = torch.rand(64,4).to(torch.device(\"cuda:0\"))\n",
        "model = LegendreMemoryUnit(1,49,4,4).to(torch.device(\"cuda:0\"))\n",
        "res, _ = model(x)\n",
        "print(res.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 5000, 49])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRuoDB59NvMX"
      },
      "source": [
        "# Models LSTM - LMU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51nVz6TsX9Fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17942982-91ef-4f53-bae1-b871fd496584"
      },
      "source": [
        "class LMUModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LMUModel, self).__init__()\n",
        "    self.LMU = LegendreMemoryUnit(1,49,4,4)\n",
        "    self.dense = nn.Linear(49,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x, _ = self.LMU(x)\n",
        "    x = self.dense(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "model_lstm = LMUModel()\n",
        "\n",
        "print(\"number of parameters : \", sum(p.numel() for p in model_lstm.parameters() if p.requires_grad))\n",
        "print()\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of parameters :  2750\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrNF6LetOGun",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61dee1f6-9845-4943-f6e8-3b93598342a6"
      },
      "source": [
        "class LSTMModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LSTMModel, self).__init__()\n",
        "    self.LSTM = nn.LSTM(1,25,1,batch_first=True)\n",
        "    self.dense = nn.Linear(25,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x, _ = self.LSTM(x)\n",
        "    x = self.dense(x)\n",
        "\n",
        "    return x\n",
        "    \n",
        "model_lstm = LSTMModel()\n",
        "\n",
        "print(\"number of parameters : \", sum(p.numel() for p in model_lstm.parameters() if p.requires_grad))\n",
        "print()\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nombre de paramètres :  2826\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soOI6lS9Ocas"
      },
      "source": [
        "# MackeyGlass Datas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOkZjfzqvhBD"
      },
      "source": [
        "Took from official LMU (tensorflow) repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voyo-rQYD-56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "599f571c-8d77-48af-ccb4-3288d4761e65"
      },
      "source": [
        "import collections \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.recurrent import LSTM, RNN\n",
        "from keras.initializers import RandomUniform\n",
        "from keras.models import Sequential\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from torchsummary import summary\n",
        "\n",
        "def mackey_glass(sample_len=1000, tau=17, delta_t=10, seed=None, n_samples=1):\n",
        "    # Adapted from https://github.com/mila-iqia/summerschool2015/blob/master/rnn_tutorial/synthetic.py\n",
        "    '''\n",
        "    mackey_glass(sample_len=1000, tau=17, seed = None, n_samples = 1) -> input\n",
        "    Generate the Mackey Glass time-series. Parameters are:\n",
        "        - sample_len: length of the time-series in timesteps. Default is 1000.\n",
        "        - tau: delay of the MG - system. Commonly used values are tau=17 (mild \n",
        "          chaos) and tau=30 (moderate chaos). Default is 17.\n",
        "        - seed: to seed the random generator, can be used to generate the same\n",
        "          timeseries at each invocation.\n",
        "        - n_samples : number of samples to generate\n",
        "    '''\n",
        "    history_len = tau * delta_t \n",
        "    # Initial conditions for the history of the system\n",
        "    timeseries = 1.2\n",
        "    \n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    samples = []\n",
        "\n",
        "    for _ in range(n_samples):\n",
        "        history = collections.deque(1.2 * np.ones(history_len) + 0.2 * \\\n",
        "                                    (np.random.rand(history_len) - 0.5))\n",
        "        # Preallocate the array for the time-series\n",
        "        inp = np.zeros((sample_len,1))\n",
        "        \n",
        "        for timestep in range(sample_len):\n",
        "            for _ in range(delta_t):\n",
        "                xtau = history.popleft()\n",
        "                history.append(timeseries)\n",
        "                timeseries = history[-1] + (0.2 * xtau / (1.0 + xtau ** 10) - \\\n",
        "                             0.1 * history[-1]) / delta_t\n",
        "            inp[timestep] = timeseries\n",
        "        \n",
        "        # Squash timeseries through tanh\n",
        "        inp = np.tanh(inp - 1)\n",
        "        samples.append(inp)\n",
        "    return samples\n",
        "\n",
        "def generate_data(n_batches, length, split=0.5, seed=0,\n",
        "                  predict_length=15, tau=17, washout=100, delta_t=1,\n",
        "                  center=True):\n",
        "    X = np.asarray(mackey_glass(\n",
        "        sample_len=length+predict_length+washout, tau=tau,\n",
        "        seed=seed, n_samples=n_batches))\n",
        "    X = X[:, washout:, :]\n",
        "    cutoff = int(split*n_batches)\n",
        "    if center:\n",
        "        X -= np.mean(X)  # global mean over all batches, approx -0.066\n",
        "    Y = X[:, predict_length:, :]\n",
        "    X = X[:, :-predict_length, :]\n",
        "    assert X.shape == Y.shape\n",
        "    return ((X[:cutoff], Y[:cutoff]),\n",
        "            (X[cutoff:], Y[cutoff:]))\n",
        "\n",
        "(train_X, train_Y), (test_X, test_Y) = generate_data(128, 5000)\n",
        "print(train_X.shape, test_X.shape)\n",
        "\n",
        "def cool_plot(X, Y, title=\"\"):\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    plt.title(title)\n",
        "    plt.scatter(X[:, 0], Y[:, 0] - X[:, 0], s=8, alpha=0.7,\n",
        "                c=np.arange(X.shape[0]), cmap=sns.cubehelix_palette(as_cmap=True))\n",
        "    plt.plot(X[:, 0], Y[:, 0] - X[:, 0], c='black', alpha=0.2)\n",
        "    plt.xlabel(\"$x(t)$\")\n",
        "    plt.ylabel(\"$y(t) - x(t)$\")\n",
        "    sns.despine(offset=15)\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "# cool_plot(train_X[0], train_Y[0])\n",
        "\n",
        "plt.plot(train_X[0][0:100])\n",
        "plt.plot(train_Y[0][0:100])\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 5000, 1) (64, 5000, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xncoH8HPLsg"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaCT4Pi_PakY"
      },
      "source": [
        "def train(model, epochs, dataset, dataset_valid = None):\n",
        "  for e in range(epochs):\n",
        "      model.train()\n",
        "      running_loss = 0\n",
        "      with tqdm(total=len(dataset)) as bar:\n",
        "        for i, (X, y) in enumerate(dataset):\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            output = model(X.cuda())\n",
        "\n",
        "            loss_ll = criterion(output, y.cuda())\n",
        "            \n",
        "            loss_ll.backward()\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss_ll.item()\n",
        "\n",
        "            bar.update(1)\n",
        "            bar.set_description(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(dataset)))\n",
        "\n",
        "      model.eval()\n",
        "      running_loss = 0\n",
        "      outs = []\n",
        "      with tqdm(total=len(dataset_valid)) as bar:\n",
        "        for i, (X, y) in enumerate(dataset_valid):\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            output = model(X.cuda())\n",
        "\n",
        "            loss_ll = criterion(output, y.cuda())\n",
        "                \n",
        "            running_loss += loss_ll.item()\n",
        "            \n",
        "            outs.append(output)\n",
        "            bar.update(1)\n",
        "            bar.set_description(\"Epoch {} - Validation loss: {}\".format(e, running_loss/len(dataset_valid)))\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEwfu8TUPTnI"
      },
      "source": [
        "# Test LMU vs LSTM on MackeyGlass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUlSXcORWqtn"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GIa1QmHpIr1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf048651-bb30-49e3-e0c7-c2326e20d625"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import torch.utils.data as data\n",
        "\n",
        "\n",
        "\n",
        "model = LMUModel().cuda()\n",
        "print(\"\\n\\number of paramaters : \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "print(\"\\n\\n\")\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "model.train()\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(torch.Tensor(train_X).cuda(), torch.Tensor(train_Y).cuda())\n",
        "dataset = data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "dataset_valid = torch.utils.data.TensorDataset(torch.Tensor(test_X).cuda(), torch.Tensor(test_Y).cuda())\n",
        "dataset_valid = data.DataLoader(dataset_valid, batch_size=16, shuffle=False)\n",
        "\n",
        "train(model, 100, dataset, dataset_valid)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "umber of paramaters :  2750\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - Training loss: 0.02681532595306635: 100%|██████████| 2/2 [00:06<00:00,  3.31s/it]\n",
            "Epoch 0 - Validation loss: 0.02019599825143814: 100%|██████████| 4/4 [00:06<00:00,  1.60s/it]\n",
            "Epoch 1 - Training loss: 0.016166726127266884: 100%|██████████| 2/2 [00:06<00:00,  3.41s/it]\n",
            "Epoch 1 - Validation loss: 0.013396643800660968: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 2 - Training loss: 0.014060364104807377: 100%|██████████| 2/2 [00:06<00:00,  3.10s/it]\n",
            "Epoch 2 - Validation loss: 0.013997412519529462: 100%|██████████| 4/4 [00:05<00:00,  1.46s/it]\n",
            "Epoch 3 - Training loss: 0.014048587530851364: 100%|██████████| 2/2 [00:06<00:00,  3.08s/it]\n",
            "Epoch 3 - Validation loss: 0.012322089169174433: 100%|██████████| 4/4 [00:05<00:00,  1.44s/it]\n",
            "Epoch 4 - Training loss: 0.011702086310833693: 100%|██████████| 2/2 [00:06<00:00,  3.30s/it]\n",
            "Epoch 4 - Validation loss: 0.010986370034515858: 100%|██████████| 4/4 [00:05<00:00,  1.49s/it]\n",
            "Epoch 5 - Training loss: 0.011442912742495537: 100%|██████████| 2/2 [00:06<00:00,  3.13s/it]\n",
            "Epoch 5 - Validation loss: 0.012264651246368885: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]\n",
            "Epoch 6 - Training loss: 0.012387938797473907: 100%|██████████| 2/2 [00:06<00:00,  3.01s/it]\n",
            "Epoch 6 - Validation loss: 0.012019595829769969: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n",
            "Epoch 7 - Training loss: 0.011847738176584244: 100%|██████████| 2/2 [00:06<00:00,  3.35s/it]\n",
            "Epoch 7 - Validation loss: 0.011045035207644105: 100%|██████████| 4/4 [00:06<00:00,  1.52s/it]\n",
            "Epoch 8 - Training loss: 0.010908204596489668: 100%|██████████| 2/2 [00:06<00:00,  3.05s/it]\n",
            "Epoch 8 - Validation loss: 0.0104171650018543: 100%|██████████| 4/4 [00:05<00:00,  1.42s/it]\n",
            "Epoch 9 - Training loss: 0.01050781924277544: 100%|██████████| 2/2 [00:05<00:00,  2.94s/it]\n",
            "Epoch 9 - Validation loss: 0.01073764730244875: 100%|██████████| 4/4 [00:05<00:00,  1.41s/it]\n",
            "Epoch 10 - Training loss: 0.010888010263442993: 100%|██████████| 2/2 [00:06<00:00,  3.42s/it]\n",
            "Epoch 10 - Validation loss: 0.010597369633615017: 100%|██████████| 4/4 [00:06<00:00,  1.58s/it]\n",
            "Epoch 11 - Training loss: 0.010483200196176767: 100%|██████████| 2/2 [00:06<00:00,  3.32s/it]\n",
            "Epoch 11 - Validation loss: 0.010045117000117898: 100%|██████████| 4/4 [00:06<00:00,  1.54s/it]\n",
            "Epoch 12 - Training loss: 0.010014961939305067: 100%|██████████| 2/2 [00:06<00:00,  3.47s/it]\n",
            "Epoch 12 - Validation loss: 0.009768001502379775: 100%|██████████| 4/4 [00:05<00:00,  1.41s/it]\n",
            "Epoch 13 - Training loss: 0.009835539385676384: 100%|██████████| 2/2 [00:06<00:00,  3.01s/it]\n",
            "Epoch 13 - Validation loss: 0.009910323191434145: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]\n",
            "Epoch 14 - Training loss: 0.009939293842762709: 100%|██████████| 2/2 [00:06<00:00,  3.11s/it]\n",
            "Epoch 14 - Validation loss: 0.009722109651193023: 100%|██████████| 4/4 [00:05<00:00,  1.49s/it]\n",
            "Epoch 15 - Training loss: 0.00973399356007576: 100%|██████████| 2/2 [00:06<00:00,  3.24s/it]\n",
            "Epoch 15 - Validation loss: 0.009484028676524758: 100%|██████████| 4/4 [00:05<00:00,  1.41s/it]\n",
            "Epoch 16 - Training loss: 0.009475435130298138: 100%|██████████| 2/2 [00:05<00:00,  2.93s/it]\n",
            "Epoch 16 - Validation loss: 0.00946911727078259: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n",
            "Epoch 17 - Training loss: 0.009486970491707325: 100%|██████████| 2/2 [00:06<00:00,  3.15s/it]\n",
            "Epoch 17 - Validation loss: 0.009502771776169538: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]\n",
            "Epoch 18 - Training loss: 0.009437044616788626: 100%|██████████| 2/2 [00:05<00:00,  2.99s/it]\n",
            "Epoch 18 - Validation loss: 0.009257004829123616: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]\n",
            "Epoch 19 - Training loss: 0.009229469113051891: 100%|██████████| 2/2 [00:05<00:00,  2.91s/it]\n",
            "Epoch 19 - Validation loss: 0.00910249468870461: 100%|██████████| 4/4 [00:05<00:00,  1.42s/it]\n",
            "Epoch 20 - Training loss: 0.009119682013988495: 100%|██████████| 2/2 [00:05<00:00,  2.95s/it]\n",
            "Epoch 20 - Validation loss: 0.00904626538977027: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]\n",
            "Epoch 21 - Training loss: 0.00908562308177352: 100%|██████████| 2/2 [00:05<00:00,  2.98s/it]\n",
            "Epoch 21 - Validation loss: 0.008936939062550664: 100%|██████████| 4/4 [00:06<00:00,  1.50s/it]\n",
            "Epoch 22 - Training loss: 0.008948783855885267: 100%|██████████| 2/2 [00:05<00:00,  2.95s/it]\n",
            "Epoch 22 - Validation loss: 0.008805806282907724: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]\n",
            "Epoch 23 - Training loss: 0.008808677550405264: 100%|██████████| 2/2 [00:05<00:00,  2.98s/it]\n",
            "Epoch 23 - Validation loss: 0.008733732858672738: 100%|██████████| 4/4 [00:05<00:00,  1.46s/it]\n",
            "Epoch 24 - Training loss: 0.008745037950575352: 100%|██████████| 2/2 [00:06<00:00,  3.17s/it]\n",
            "Epoch 24 - Validation loss: 0.008628541603684425: 100%|██████████| 4/4 [00:06<00:00,  1.58s/it]\n",
            "Epoch 25 - Training loss: 0.00860398355871439: 100%|██████████| 2/2 [00:06<00:00,  3.17s/it]\n",
            "Epoch 25 - Validation loss: 0.00846369843930006: 100%|██████████| 4/4 [00:06<00:00,  1.68s/it]\n",
            "Epoch 26 - Training loss: 0.008474072441458702: 100%|██████████| 2/2 [00:06<00:00,  3.05s/it]\n",
            "Epoch 26 - Validation loss: 0.008387794950976968: 100%|██████████| 4/4 [00:05<00:00,  1.42s/it]\n",
            "Epoch 27 - Training loss: 0.008396512363106012: 100%|██████████| 2/2 [00:05<00:00,  2.93s/it]\n",
            "Epoch 27 - Validation loss: 0.008280032547190785: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n",
            "Epoch 28 - Training loss: 0.008287237957119942: 100%|██████████| 2/2 [00:06<00:00,  3.18s/it]\n",
            "Epoch 28 - Validation loss: 0.008173257810994983: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 29 - Training loss: 0.008183017373085022: 100%|██████████| 2/2 [00:06<00:00,  3.16s/it]\n",
            "Epoch 29 - Validation loss: 0.008091937052085996: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]\n",
            "Epoch 30 - Training loss: 0.008083484135568142: 100%|██████████| 2/2 [00:06<00:00,  3.08s/it]\n",
            "Epoch 30 - Validation loss: 0.007975410437211394: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n",
            "Epoch 31 - Training loss: 0.007985922507941723: 100%|██████████| 2/2 [00:06<00:00,  3.02s/it]\n",
            "Epoch 31 - Validation loss: 0.007900025695562363: 100%|██████████| 4/4 [00:06<00:00,  1.57s/it]\n",
            "Epoch 32 - Training loss: 0.007906022015959024: 100%|██████████| 2/2 [00:06<00:00,  3.50s/it]\n",
            "Epoch 32 - Validation loss: 0.007798365666531026: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 33 - Training loss: 0.007807926274836063: 100%|██████████| 2/2 [00:06<00:00,  3.11s/it]\n",
            "Epoch 33 - Validation loss: 0.0077192934695631266: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 34 - Training loss: 0.007736204424872994: 100%|██████████| 2/2 [00:06<00:00,  3.16s/it]\n",
            "Epoch 34 - Validation loss: 0.0076392225455492735: 100%|██████████| 4/4 [00:05<00:00,  1.44s/it]\n",
            "Epoch 35 - Training loss: 0.00764695811085403: 100%|██████████| 2/2 [00:06<00:00,  3.05s/it]\n",
            "Epoch 35 - Validation loss: 0.0075552467023953795: 100%|██████████| 4/4 [00:06<00:00,  1.66s/it]\n",
            "Epoch 36 - Training loss: 0.007564092054963112: 100%|██████████| 2/2 [00:06<00:00,  3.38s/it]\n",
            "Epoch 36 - Validation loss: 0.007474242825992405: 100%|██████████| 4/4 [00:06<00:00,  1.57s/it]\n",
            "Epoch 37 - Training loss: 0.007489360636100173: 100%|██████████| 2/2 [00:06<00:00,  3.11s/it]\n",
            "Epoch 37 - Validation loss: 0.007397285895422101: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n",
            "Epoch 38 - Training loss: 0.007410174701362848: 100%|██████████| 2/2 [00:06<00:00,  3.22s/it]\n",
            "Epoch 38 - Validation loss: 0.00731319619808346: 100%|██████████| 4/4 [00:06<00:00,  1.59s/it]\n",
            "Epoch 39 - Training loss: 0.007326047169044614: 100%|██████████| 2/2 [00:06<00:00,  3.25s/it]\n",
            "Epoch 39 - Validation loss: 0.007241896470077336: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 40 - Training loss: 0.007249719696119428: 100%|██████████| 2/2 [00:05<00:00,  2.93s/it]\n",
            "Epoch 40 - Validation loss: 0.007153647602535784: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 41 - Training loss: 0.007168328622356057: 100%|██████████| 2/2 [00:05<00:00,  2.99s/it]\n",
            "Epoch 41 - Validation loss: 0.007077758084051311: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 42 - Training loss: 0.007092776242643595: 100%|██████████| 2/2 [00:06<00:00,  3.47s/it]\n",
            "Epoch 42 - Validation loss: 0.006996831973083317: 100%|██████████| 4/4 [00:06<00:00,  1.61s/it]\n",
            "Epoch 43 - Training loss: 0.007016948889940977: 100%|██████████| 2/2 [00:05<00:00,  2.98s/it]\n",
            "Epoch 43 - Validation loss: 0.006934825098142028: 100%|██████████| 4/4 [00:05<00:00,  1.46s/it]\n",
            "Epoch 44 - Training loss: 0.006940749241039157: 100%|██████████| 2/2 [00:06<00:00,  3.09s/it]\n",
            "Epoch 44 - Validation loss: 0.006889406940899789: 100%|██████████| 4/4 [00:06<00:00,  1.51s/it]\n",
            "Epoch 45 - Training loss: 0.006910002790391445: 100%|██████████| 2/2 [00:06<00:00,  3.32s/it]\n",
            "Epoch 45 - Validation loss: 0.006760286167263985: 100%|██████████| 4/4 [00:06<00:00,  1.57s/it]\n",
            "Epoch 46 - Training loss: 0.006792054511606693: 100%|██████████| 2/2 [00:06<00:00,  3.19s/it]\n",
            "Epoch 46 - Validation loss: 0.006734977243468165: 100%|██████████| 4/4 [00:05<00:00,  1.46s/it]\n",
            "Epoch 47 - Training loss: 0.006765234051272273: 100%|██████████| 2/2 [00:06<00:00,  3.07s/it]\n",
            "Epoch 47 - Validation loss: 0.006655449280515313: 100%|██████████| 4/4 [00:05<00:00,  1.49s/it]\n",
            "Epoch 48 - Training loss: 0.00664122449234128: 100%|██████████| 2/2 [00:06<00:00,  3.35s/it]\n",
            "Epoch 48 - Validation loss: 0.006594375940039754: 100%|██████████| 4/4 [00:06<00:00,  1.61s/it]\n",
            "Epoch 49 - Training loss: 0.00669764238409698: 100%|██████████| 2/2 [00:06<00:00,  3.21s/it]\n",
            "Epoch 49 - Validation loss: 0.006715696887113154: 100%|██████████| 4/4 [00:05<00:00,  1.46s/it]\n",
            "Epoch 50 - Training loss: 0.006677904631942511: 100%|██████████| 2/2 [00:05<00:00,  2.95s/it]\n",
            "Epoch 50 - Validation loss: 0.006367737543769181: 100%|██████████| 4/4 [00:05<00:00,  1.46s/it]\n",
            "Epoch 51 - Training loss: 0.006420243997126818: 100%|██████████| 2/2 [00:06<00:00,  3.05s/it]\n",
            "Epoch 51 - Validation loss: 0.00649591360706836: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n",
            "Epoch 52 - Training loss: 0.006528413621708751: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]\n",
            "Epoch 52 - Validation loss: 0.006332023884169757: 100%|██████████| 4/4 [00:06<00:00,  1.59s/it]\n",
            "Epoch 53 - Training loss: 0.006286960560828447: 100%|██████████| 2/2 [00:05<00:00,  2.94s/it]\n",
            "Epoch 53 - Validation loss: 0.006146034924313426: 100%|██████████| 4/4 [00:05<00:00,  1.41s/it]\n",
            "Epoch 54 - Training loss: 0.006238785805180669: 100%|██████████| 2/2 [00:06<00:00,  3.13s/it]\n",
            "Epoch 54 - Validation loss: 0.006439284537918866: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 55 - Training loss: 0.006494248053058982: 100%|██████████| 2/2 [00:06<00:00,  3.04s/it]\n",
            "Epoch 55 - Validation loss: 0.006410193862393498: 100%|██████████| 4/4 [00:06<00:00,  1.61s/it]\n",
            "Epoch 56 - Training loss: 0.00631895731203258: 100%|██████████| 2/2 [00:06<00:00,  3.38s/it]\n",
            "Epoch 56 - Validation loss: 0.0059003999922424555: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n",
            "Epoch 57 - Training loss: 0.005935033550485969: 100%|██████████| 2/2 [00:06<00:00,  3.10s/it]\n",
            "Epoch 57 - Validation loss: 0.006076124147512019: 100%|██████████| 4/4 [00:06<00:00,  1.51s/it]\n",
            "Epoch 58 - Training loss: 0.006200371775776148: 100%|██████████| 2/2 [00:06<00:00,  3.23s/it]\n",
            "Epoch 58 - Validation loss: 0.006314323050901294: 100%|██████████| 4/4 [00:06<00:00,  1.51s/it]\n",
            "Epoch 59 - Training loss: 0.006315493723377585: 100%|██████████| 2/2 [00:06<00:00,  3.46s/it]\n",
            "Epoch 59 - Validation loss: 0.005918555427342653: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]\n",
            "Epoch 60 - Training loss: 0.005790394498035312: 100%|██████████| 2/2 [00:06<00:00,  3.07s/it]\n",
            "Epoch 60 - Validation loss: 0.0056936233304440975: 100%|██████████| 4/4 [00:05<00:00,  1.49s/it]\n",
            "Epoch 61 - Training loss: 0.005933771841228008: 100%|██████████| 2/2 [00:06<00:00,  3.02s/it]\n",
            "Epoch 61 - Validation loss: 0.006600418710149825: 100%|██████████| 4/4 [00:05<00:00,  1.50s/it]\n",
            "Epoch 62 - Training loss: 0.006573471240699291: 100%|██████████| 2/2 [00:06<00:00,  3.06s/it]\n",
            "Epoch 62 - Validation loss: 0.005739625776186585: 100%|██████████| 4/4 [00:06<00:00,  1.61s/it]\n",
            "Epoch 63 - Training loss: 0.005631167674437165: 100%|██████████| 2/2 [00:06<00:00,  3.34s/it]\n",
            "Epoch 63 - Validation loss: 0.005957258632406592: 100%|██████████| 4/4 [00:05<00:00,  1.42s/it]\n",
            "Epoch 64 - Training loss: 0.005960291251540184: 100%|██████████| 2/2 [00:06<00:00,  3.15s/it]\n",
            "Epoch 64 - Validation loss: 0.005379373207688332: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]\n",
            "Epoch 65 - Training loss: 0.005392070626839995: 100%|██████████| 2/2 [00:06<00:00,  3.22s/it]\n",
            "Epoch 65 - Validation loss: 0.0058648273115977645: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n",
            "Epoch 66 - Training loss: 0.006040862062945962: 100%|██████████| 2/2 [00:06<00:00,  3.14s/it]\n",
            "Epoch 66 - Validation loss: 0.005803039064630866: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]\n",
            "Epoch 67 - Training loss: 0.005517867160961032: 100%|██████████| 2/2 [00:06<00:00,  3.05s/it]\n",
            "Epoch 67 - Validation loss: 0.005008473992347717: 100%|██████████| 4/4 [00:05<00:00,  1.45s/it]\n",
            "Epoch 68 - Training loss: 0.00525042787194252: 100%|██████████| 2/2 [00:06<00:00,  3.08s/it]\n",
            "Epoch 68 - Validation loss: 0.005682391347363591: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]\n",
            "Epoch 69 - Training loss: 0.005609432002529502: 100%|██████████| 2/2 [00:06<00:00,  3.03s/it]\n",
            "Epoch 69 - Validation loss: 0.004818609566427767: 100%|██████████| 4/4 [00:06<00:00,  1.52s/it]\n",
            "Epoch 70 - Training loss: 0.004824547097086906: 100%|██████████| 2/2 [00:05<00:00,  2.83s/it]\n",
            "Epoch 70 - Validation loss: 0.005204638000577688: 100%|██████████| 4/4 [00:06<00:00,  1.57s/it]\n",
            "Epoch 71 - Training loss: 0.00527819711714983: 100%|██████████| 2/2 [00:06<00:00,  3.12s/it]\n",
            "Epoch 71 - Validation loss: 0.0025384777691215277:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-41632912c113>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdataset_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-56a3ed0afd1c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, dataset, dataset_valid)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mloss_ll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-ca8d2095a27a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLMU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-f4125ed53047>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xt)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m       \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlmucell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-f4125ed53047>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xt, states)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mht\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mht\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m  \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qaunc4vo9Orw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "82bc2c3f-3470-48b4-c140-9c6e612cc5a5"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import torch.utils.data as data\n",
        "\n",
        "\n",
        "\n",
        "model = LSTMModel().cuda()\n",
        "print(\"\\nnumber of paramaters : \", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "print(\"\\n\\n\")\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "model.train()\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(torch.Tensor(train_X).cuda(), torch.Tensor(train_Y).cuda())\n",
        "dataset = data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "dataset_valid = torch.utils.data.TensorDataset(torch.Tensor(test_X).cuda(), torch.Tensor(test_Y).cuda())\n",
        "dataset_valid = data.DataLoader(dataset_valid, batch_size=16, shuffle=False)\n",
        "\n",
        "train(model, 100, dataset, dataset_valid)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - Training loss: 0.012470325455069542:  25%|██▌       | 1/4 [00:00<00:00,  5.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "number of paramaters :  2826\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 - Training loss: 0.04816618748009205: 100%|██████████| 4/4 [00:00<00:00,  7.27it/s]\n",
            "Epoch 0 - Validation loss: 0.04590757563710213: 100%|██████████| 4/4 [00:00<00:00, 18.52it/s]\n",
            "Epoch 1 - Training loss: 0.04513815324753523: 100%|██████████| 4/4 [00:00<00:00,  9.68it/s]\n",
            "Epoch 1 - Validation loss: 0.04424694087356329: 100%|██████████| 4/4 [00:00<00:00, 18.11it/s]\n",
            "Epoch 2 - Training loss: 0.044021015986800194: 100%|██████████| 4/4 [00:00<00:00,  8.30it/s]\n",
            "Epoch 2 - Validation loss: 0.04376828204840422: 100%|██████████| 4/4 [00:00<00:00, 14.90it/s]\n",
            "Epoch 3 - Training loss: 0.0436081662774086: 100%|██████████| 4/4 [00:00<00:00,  7.89it/s]\n",
            "Epoch 3 - Validation loss: 0.04326228145509958: 100%|██████████| 4/4 [00:00<00:00, 18.46it/s]\n",
            "Epoch 4 - Training loss: 0.04292771592736244: 100%|██████████| 4/4 [00:00<00:00,  9.81it/s]\n",
            "Epoch 4 - Validation loss: 0.042302656918764114: 100%|██████████| 4/4 [00:00<00:00, 19.22it/s]\n",
            "Epoch 5 - Training loss: 0.04188293777406216: 100%|██████████| 4/4 [00:00<00:00,  8.96it/s]\n",
            "Epoch 5 - Validation loss: 0.041190480813384056: 100%|██████████| 4/4 [00:00<00:00, 19.54it/s]\n",
            "Epoch 6 - Training loss: 0.04079908598214388: 100%|██████████| 4/4 [00:00<00:00,  9.32it/s]\n",
            "Epoch 6 - Validation loss: 0.040138861164450645: 100%|██████████| 4/4 [00:00<00:00, 16.69it/s]\n",
            "Epoch 7 - Training loss: 0.03971670474857092: 100%|██████████| 4/4 [00:00<00:00,  9.38it/s]\n",
            "Epoch 7 - Validation loss: 0.03891334589570761: 100%|██████████| 4/4 [00:00<00:00, 19.60it/s]\n",
            "Epoch 8 - Training loss: 0.03833235427737236: 100%|██████████| 4/4 [00:00<00:00,  9.01it/s]\n",
            "Epoch 8 - Validation loss: 0.037227045744657516: 100%|██████████| 4/4 [00:00<00:00, 18.92it/s]\n",
            "Epoch 9 - Training loss: 0.0364748639985919: 100%|██████████| 4/4 [00:00<00:00,  9.60it/s]\n",
            "Epoch 9 - Validation loss: 0.03507460840046406: 100%|██████████| 4/4 [00:00<00:00, 19.35it/s]\n",
            "Epoch 10 - Training loss: 0.03410259075462818: 100%|██████████| 4/4 [00:00<00:00,  9.67it/s]\n",
            "Epoch 10 - Validation loss: 0.03219646215438843: 100%|██████████| 4/4 [00:00<00:00, 19.53it/s]\n",
            "Epoch 11 - Training loss: 0.030772509053349495: 100%|██████████| 4/4 [00:00<00:00,  9.37it/s]\n",
            "Epoch 11 - Validation loss: 0.02798115322366357: 100%|██████████| 4/4 [00:00<00:00, 18.51it/s]\n",
            "Epoch 12 - Training loss: 0.025941242463886738: 100%|██████████| 4/4 [00:00<00:00,  9.84it/s]\n",
            "Epoch 12 - Validation loss: 0.021952814888209105: 100%|██████████| 4/4 [00:00<00:00, 15.89it/s]\n",
            "Epoch 13 - Training loss: 0.01917364913970232: 100%|██████████| 4/4 [00:00<00:00,  9.41it/s]\n",
            "Epoch 13 - Validation loss: 0.014312632847577333: 100%|██████████| 4/4 [00:00<00:00, 19.08it/s]\n",
            "Epoch 14 - Training loss: 0.012273649219423532: 100%|██████████| 4/4 [00:00<00:00,  9.75it/s]\n",
            "Epoch 14 - Validation loss: 0.010529619408771396: 100%|██████████| 4/4 [00:00<00:00, 19.05it/s]\n",
            "Epoch 15 - Training loss: 0.012414639815688133: 100%|██████████| 4/4 [00:00<00:00,  9.72it/s]\n",
            "Epoch 15 - Validation loss: 0.013181648682802916: 100%|██████████| 4/4 [00:00<00:00, 15.60it/s]\n",
            "Epoch 16 - Training loss: 0.01265149051323533: 100%|██████████| 4/4 [00:00<00:00,  8.26it/s]\n",
            "Epoch 16 - Validation loss: 0.011444202158600092: 100%|██████████| 4/4 [00:00<00:00, 16.48it/s]\n",
            "Epoch 17 - Training loss: 0.01031387341208756: 100%|██████████| 4/4 [00:00<00:00,  8.43it/s]\n",
            "Epoch 17 - Validation loss: 0.0098161103669554: 100%|██████████| 4/4 [00:00<00:00, 18.46it/s]\n",
            "Epoch 18 - Training loss: 0.009429044555872679: 100%|██████████| 4/4 [00:00<00:00,  9.52it/s]\n",
            "Epoch 18 - Validation loss: 0.009324232349172235: 100%|██████████| 4/4 [00:00<00:00, 15.01it/s]\n",
            "Epoch 19 - Training loss: 0.009437397820875049: 100%|██████████| 4/4 [00:00<00:00,  8.66it/s]\n",
            "Epoch 19 - Validation loss: 0.009241001680493355: 100%|██████████| 4/4 [00:00<00:00, 15.15it/s]\n",
            "Epoch 20 - Training loss: 0.009272074792534113: 100%|██████████| 4/4 [00:00<00:00,  8.08it/s]\n",
            "Epoch 20 - Validation loss: 0.009164247661828995: 100%|██████████| 4/4 [00:00<00:00, 19.12it/s]\n",
            "Epoch 21 - Training loss: 0.009004481369629502: 100%|██████████| 4/4 [00:00<00:00,  9.63it/s]\n",
            "Epoch 21 - Validation loss: 0.008665013127028942: 100%|██████████| 4/4 [00:00<00:00, 18.98it/s]\n",
            "Epoch 22 - Training loss: 0.008601427543908358: 100%|██████████| 4/4 [00:00<00:00,  9.38it/s]\n",
            "Epoch 22 - Validation loss: 0.008314208826050162: 100%|██████████| 4/4 [00:00<00:00, 16.95it/s]\n",
            "Epoch 23 - Training loss: 0.008276684209704399: 100%|██████████| 4/4 [00:00<00:00,  9.39it/s]\n",
            "Epoch 23 - Validation loss: 0.00816367007791996: 100%|██████████| 4/4 [00:00<00:00, 18.36it/s]\n",
            "Epoch 24 - Training loss: 0.008140451274812222: 100%|██████████| 4/4 [00:00<00:00,  8.55it/s]\n",
            "Epoch 24 - Validation loss: 0.008087181020528078: 100%|██████████| 4/4 [00:00<00:00, 15.21it/s]\n",
            "Epoch 25 - Training loss: 0.008064445573836565: 100%|██████████| 4/4 [00:00<00:00,  8.23it/s]\n",
            "Epoch 25 - Validation loss: 0.007988769561052322: 100%|██████████| 4/4 [00:00<00:00, 15.94it/s]\n",
            "Epoch 26 - Training loss: 0.00796223827637732: 100%|██████████| 4/4 [00:00<00:00,  8.43it/s]\n",
            "Epoch 26 - Validation loss: 0.00786855909973383: 100%|██████████| 4/4 [00:00<00:00, 18.34it/s]\n",
            "Epoch 27 - Training loss: 0.007848369423300028: 100%|██████████| 4/4 [00:00<00:00,  8.38it/s]\n",
            "Epoch 27 - Validation loss: 0.007762292283587158: 100%|██████████| 4/4 [00:00<00:00, 13.85it/s]\n",
            "Epoch 28 - Training loss: 0.0077587703708559275: 100%|██████████| 4/4 [00:00<00:00,  7.77it/s]\n",
            "Epoch 28 - Validation loss: 0.007695948821492493: 100%|██████████| 4/4 [00:00<00:00, 14.68it/s]\n",
            "Epoch 29 - Training loss: 0.007697661756537855: 100%|██████████| 4/4 [00:00<00:00,  9.45it/s]\n",
            "Epoch 29 - Validation loss: 0.007647270103916526: 100%|██████████| 4/4 [00:00<00:00, 16.86it/s]\n",
            "Epoch 30 - Training loss: 0.007645303499884903: 100%|██████████| 4/4 [00:00<00:00,  9.69it/s]\n",
            "Epoch 30 - Validation loss: 0.007594967493787408: 100%|██████████| 4/4 [00:00<00:00, 18.86it/s]\n",
            "Epoch 31 - Training loss: 0.007590636145323515: 100%|██████████| 4/4 [00:00<00:00,  9.51it/s]\n",
            "Epoch 31 - Validation loss: 0.007543676299974322: 100%|██████████| 4/4 [00:00<00:00, 19.08it/s]\n",
            "Epoch 32 - Training loss: 0.007538712932728231: 100%|██████████| 4/4 [00:00<00:00,  9.47it/s]\n",
            "Epoch 32 - Validation loss: 0.007496502948924899: 100%|██████████| 4/4 [00:00<00:00, 19.54it/s]\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-9345e03a8170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdataset_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-56a3ed0afd1c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, dataset, dataset_valid)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mloss_ll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mloss_ll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}